{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Variable.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNLPDMfOaGtLInI4Rzz3p5o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Pytorch Tensor\n","\n","在 pytorch 這個框架中，所有的運算、資料流動或是其他數學方法的底層都是張量 (tensor)，pytorch tensor 有著和 Numpy 同樣方便的函數以及操作方法，基本上我們可以直接套用 numpy 的邏輯到 tensor 處理上面，除此之外，也有很多類似函數提供使用。"],"metadata":{"id":"lW0CRstQo_FY"}},{"cell_type":"markdown","source":["### Build a tensor \n","\n","如何建立一個 tensor，又要怎麼把 tensor 移到裝置上面呢? [pytorch tensor](https://pytorch.org/docs/stable/tensors.html)"],"metadata":{"id":"IzWZUwwKrd1c"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUP-dWA5mXku","executionInfo":{"status":"ok","timestamp":1657425141147,"user_tz":-480,"elapsed":3327,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"ee8874bb-ecb8-48e1-baee-372f637a650b"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor1: tensor([[1, 2],\n","        [3, 4]]) \n","tensor2: tensor([[1., 2.],\n","        [3., 4.]]) \n","tensor3: tensor([[1, 2],\n","        [3, 4]], dtype=torch.uint8) \n","tensor4: tensor([[1., 2.],\n","        [3., 4.]])\n"]}],"source":["import torch # import pytorch library\n","\n","tensor1 = torch.tensor([[1, 2], [3, 4]])\n","tensor2 = torch.FloatTensor([[1, 2], [3, 4]])\n","\n","tensor3 = torch.ByteTensor([[1, 2], [3, 4]]) # You may see the data type in cv2 preprocessing\n","tensor4 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) # Same as tensor1\n","\n","print(\n","    f'tensor1: {tensor1}',\n","    f'\\ntensor2: {tensor2}',\n","    f'\\ntensor3: {tensor3}',\n","    f'\\ntensor4: {tensor4}'\n","    )"]},{"cell_type":"markdown","source":["## Device\n","\n","在 tensor 的計算中，我們常常需要非常大量的運算，這些運算之間分別可以以多線程進行，所以一種有效率的方法是把任務分配到不同的 block 和 thread 上。\n","\n","如果你使用的是 numpy array 物件，那麼你可以用 cupy 套件的提供的 built-in 函數將資料移到 device 上訓練\n","\n","如果你使用的是 pytorch tensor 物件，pytorch 可以很輕鬆的把所有資料都移到 device  上訓練，但要注意記憶體容量限制\n","\n","- host: cpu\n","- device: gpu\n","\n","如果我想把 tensor 建立在 device 上呢? \n","\n","pytorch 支援 cuda 加速，所以我們可以在 cuda 上建立 tensor，或者在 cpu 上建立 tensor 後移到 cuda 上運算"],"metadata":{"id":"c17VLbc5tZxq"}},{"cell_type":"code","source":["##### Remember to initialize your GPU setting to prevent RuntimeError alert\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","tensor1 = torch.tensor([[1, 2], [3, 4]])\n","tensor1 = tensor1.to(DEVICE)\n","#tensor1 = tensor1.cuda()\n","\n","tensor2 = torch.cuda.FloatTensor([[1, 2], [3, 4]])\n","\n","tensor3 = torch.ByteTensor([[1, 2], [3, 4]]) # You may see the data type in cv2 preprocessing\n","tensor4 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) # Same as tensor1\n","\n","print(\n","    f'DEVICE: {DEVICE}'\n","    f'\\ntensor1: {tensor1}',\n","    f'\\ntensor2: {tensor2}',\n","    f'\\ntensor3: {tensor3}',\n","    f'\\ntensor4: {tensor4}'\n","    )\n","\n","#You may see this error occur if the operation of tensors are not on the same divece\n","try:\n","    tensor2 + tensor3\n","except Exception as error: \n","    print(error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6eSE7Kao8o2","executionInfo":{"status":"ok","timestamp":1657425154309,"user_tz":-480,"elapsed":13165,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"21dda66b-45c7-464a-ddd5-d5345199a1ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DEVICE: cuda\n","tensor1: tensor([[1, 2],\n","        [3, 4]], device='cuda:0') \n","tensor2: tensor([[1., 2.],\n","        [3., 4.]], device='cuda:0') \n","tensor3: tensor([[1, 2],\n","        [3, 4]], dtype=torch.uint8) \n","tensor4: tensor([[1., 2.],\n","        [3., 4.]])\n","Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"]}]},{"cell_type":"markdown","source":["### Variable\n","\n","每一個 tensor 都可以看成是\"常數\"，他們無法進行反向傳播或者運算關係之間的連接，所以我們要用 Variable 方法，將 tensor 註冊到變數中進行梯度的反向傳播\n","\n","Variable 可以看成是一張圖，這張圖包含了所有被註冊到 Variable 底下的 tensors 以及 tensors 之間的運算關係，因為 Variable 類包含了運算關係，所以我們只要調用 Variable 底下的 ```.backward()``` 就可以進行反向傳播，不需要做其他任何的額外操作"],"metadata":{"id":"SX0kZaTVwQ94"}},{"cell_type":"code","source":["from torch.autograd import Variable\n","\n","var =  Variable(tensor4, requires_grad=True) # The default value of 'requires_grad' is False, make sure to set it True !!\n","tensor5 = torch.mean(var * var)\n","\n","tensor5.backward() # back propagation\n","\n","print(\n","    f'variable:\\n {var}'\n","    f'\\ngradient:\\n {var.grad}', # The gradient of 1 / n ** 2 * sum(X * X) = 2 / n ** 2 * sum(X) = 2 / 4 * X = 0.5 * X\n","    f'\\ndata:\\n {var.data}', # collect your data\n","    f'\\nto numpy:\\n {var.data.numpy()}', # collect your data then to numpy\n","    f'\\ndetach tensor:\\n {var.detach()}' # detach tensor from training (requires_grad=False)\n","    )\n","\n","tensor6 = tensor4.to(DEVICE)\n","var =  Variable(tensor6, requires_grad=True) # The default value of 'requires_grad' is False, make sure to set it True !!\n","tensor5 = torch.mean(var * var)\n","\n","tensor5.backward() # back propagation\n","\n","try:\n","    var.data.numpy()\n","except Exception as error:\n","    print('\\n\\n', error)\n","    print(f'move to host memory: {var.data.cpu().numpy()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9xYFUgVwORf","executionInfo":{"status":"ok","timestamp":1657425154310,"user_tz":-480,"elapsed":8,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"51dc9615-23a4-4db3-f4ec-e77baf79a68a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["variable:\n"," tensor([[1., 2.],\n","        [3., 4.]], requires_grad=True)\n","gradient:\n"," tensor([[0.5000, 1.0000],\n","        [1.5000, 2.0000]]) \n","data:\n"," tensor([[1., 2.],\n","        [3., 4.]]) \n","to numpy:\n"," [[1. 2.]\n"," [3. 4.]] \n","detach tensor:\n"," tensor([[1., 2.],\n","        [3., 4.]])\n","\n","\n"," can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n","move to host memory: [[1. 2.]\n"," [3. 4.]]\n"]}]},{"cell_type":"markdown","source":["### Define your constant and variable in Neural Network\n","\n","在 torch.nn.Module 用來定義常量與變數的方法是用 ```nn.Parameter()``` 而不是 ```Variable```\n","\n","注意: Variable 並不會放到網路中，所以在 optimizer 更新時不會更新 Variable 參數\n","\n","- ```nn.Parameter():``` 將參數放在網路中，同時設定 ```requires_grad = True```\n","- ```register_buffer():``` 在網路中設定常量，並不會在 ```step()``` 方法中更新參數\n","- ```register_parameter():``` 在網路中設定變量，會在 ```step()``` 方法中更新參數"],"metadata":{"id":"QB8HTGl83rP2"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class MyModule(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.variable = Variable(torch.Tensor([5]))\n","        self.parameter = nn.Parameter(torch.Tensor([10]))\n","        self.register_buffer('buffer', torch.Tensor([15]))\n","        self.register_parameter('register_param', self.parameter)\n","\n","    def forward(self, x):\n","\n","        print('\\n\\nbuffer:\\n', self.buffer)\n","        print('\\n', self.parameter)\n","        print('\\nvariable:\\n', self.variable)\n","\n","        out = x + self.parameter + self.parameter + self.buffer\n","        out.backward()\n","\n","        print('\\ngradient for parameter:\\n', self.parameter.grad)\n","        print('\\ngradient for buffer:\\n', self.buffer.grad)\n","        print('\\nIf parameter require gradient:\\n', self.register_param.requires_grad)\n","        print('\\nIf buffer require gradient:\\n', self.buffer.requires_grad)\n","\n","        return out\n","\n","net = MyModule()\n","for param in net.parameters():\n","    print(param)\n","out = net.forward(1)\n","out"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTTxWyztzGX2","executionInfo":{"status":"ok","timestamp":1657425367214,"user_tz":-480,"elapsed":800,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"f4d07b0a-2fc2-4d4b-9f0a-5e82a3f4055c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([10.], requires_grad=True)\n","\n","\n","buffer:\n"," tensor([15.])\n","\n"," Parameter containing:\n","tensor([10.], requires_grad=True)\n","\n","variable:\n"," tensor([5.])\n","\n","gradient for parameter:\n"," tensor([2.])\n","\n","gradient for buffer:\n"," None\n","\n","If parameter require gradient:\n"," True\n","\n","If buffer require gradient:\n"," False\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([36.], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## Tensor operation\n","\n","這裡重點介紹一些重要的 tensor 運算函數\n","\n","- ```torch.cat():``` 將一個 list 的 tensor 進行串接\n","- ```x.view():``` 將 tensor 的維度重建，同 reshape\n","- ```x.permute():``` 將 tensor 的維度做調換 "],"metadata":{"id":"_oPJi-h6553q"}},{"cell_type":"code","source":["x = torch.Tensor([[1, 2], [3, 4]])\n","y = torch.Tensor([[5, 6], [7, 8]])\n","\n","print('concatenate:\\n', torch.cat([x, y], axis=1))\n","print('reshape:\\n', x.view(1, 4))\n","print('permute:\\n', x.permute(1, 0))"],"metadata":{"id":"aTDjk_Js56qU"},"execution_count":null,"outputs":[]}]}