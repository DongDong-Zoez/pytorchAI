{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module.ipynb","provenance":[],"authorship_tag":"ABX9TyNSa7N2OR4/VK4qKiAVUJ/Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Pytorch Module\n","\n","pytorch module 是一個 pytorch 寫好的物件，當我們想要自定義網路的時候必須從 ```nn.Module``` 繼承 pytorch module 的屬性，在自定義網路中必須包含兩個部分\n","\n","- ```__init__():``` 網路初始化的設定\n","- ```forward():``` 前向傳播的方法，回傳模型的輸出\n","\n","另外 nn 底下的 built-in 函數都會自動放在 ```nn.Parameter``` 裡面，所以不用特定再聲明\n","\n","我們先來看一個非常簡單的例子"],"metadata":{"id":"nTzbflog6ZH5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haEyjS4i6TZQ","executionInfo":{"status":"ok","timestamp":1654839393232,"user_tz":-480,"elapsed":298,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"5eb19130-b67a-460d-8fc0-ef821a3a3b2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["output size: torch.Size([3, 64])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                [-1, 3, 64]           8,256\n","           Sigmoid-2                [-1, 3, 64]               0\n","================================================================\n","Total params: 8,256\n","Trainable params: 8,256\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.03\n","Estimated Total Size (MB): 0.04\n","----------------------------------------------------------------\n","MyModule(\n","  (weight): Linear(in_features=128, out_features=64, bias=True)\n","  (activation): Sigmoid()\n",")\n"]}],"source":["import torch.nn as nn\n","import torch\n","\n","from torchsummary import summary\n","\n","class MyModule(nn.Module):\n","\n","    def __init__(self, in_features=128, out_features=64):\n","        super().__init__()\n","\n","        self.weight = nn.Linear(in_features, out_features)\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.weight(x)\n","        x = self.activation(x)\n","        return x\n","\n","if __name__ == '__main__':\n","    x = torch.randn(3,128)\n","    module = MyModule()\n","    print('output size:', module(x).shape)\n","    summary(module, (3, 128))\n","    print(module)"]},{"cell_type":"markdown","source":["在這個例子中我們的網路有兩層\n","\n","- weight: 會將 input data 乘上一個大小為 128 * 64 的矩陣，再加上維度為 64 的 bias 向量\n","- activation: 將 Linear 層的 output 通過一個 sigmoid 函數，保證 output 的值落在 [0, 1] 區間內\n","\n","全部參數量 8256 是怎麼計算出來的呢?\n","\n","- 網路的計算是 $\\sigma(WX+B)$，其中 $W、B$ 是可更新參數\n","- 所以全部參數量為 64 * 128 + 64\n"],"metadata":{"id":"taos9eQs87Nu"}},{"cell_type":"markdown","source":["## Sequential\n","\n","如果我們想要更多層的神經網路呢? 我們可以使用 Sequential 方法，Sequential 會將一連串的函數傳接再一起，並且自動生成計算圖的路徑 (自動生成 forward 路徑)"],"metadata":{"id":"AeleE7Wa-GGx"}},{"cell_type":"code","source":["class MyModule(nn.Module):\n","\n","    def __init__(self, in_features=128, out_features=64):\n","        super().__init__()\n","\n","        self.layers = nn.Sequential(\n","            nn.Linear(in_features, in_features // 2),\n","            nn.BatchNorm1d(in_features // 2),\n","            nn.ReLU(),\n","            nn.Linear(in_features // 2, out_features),\n","            nn.BatchNorm1d(out_features),\n","        )\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.activation(x)\n","        return x\n","\n","if __name__ == '__main__':\n","    x = torch.randn(64,128)\n","    module = MyModule()\n","    print('output size:', module(x).shape)\n","    summary(module, (64, 128))\n","    print(module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-f5pBf9975D","executionInfo":{"status":"ok","timestamp":1654839397771,"user_tz":-480,"elapsed":278,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"ce2a2472-1c8d-4d80-a09a-ca8c2b1240a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output size: torch.Size([64, 64])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1               [-1, 64, 64]           8,256\n","       BatchNorm1d-2               [-1, 64, 64]             128\n","              ReLU-3               [-1, 64, 64]               0\n","            Linear-4               [-1, 64, 64]           4,160\n","       BatchNorm1d-5               [-1, 64, 64]             128\n","           Sigmoid-6               [-1, 64, 64]               0\n","================================================================\n","Total params: 12,672\n","Trainable params: 12,672\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 0.19\n","Params size (MB): 0.05\n","Estimated Total Size (MB): 0.27\n","----------------------------------------------------------------\n","MyModule(\n","  (layers): Sequential(\n","    (0): Linear(in_features=128, out_features=64, bias=True)\n","    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Linear(in_features=64, out_features=64, bias=True)\n","    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (activation): Sigmoid()\n",")\n"]}]},{"cell_type":"markdown","source":["除了直接寫在 Sequential 裡面，我們也可以寫在 List 裡面再展開"],"metadata":{"id":"USJjvkuu_9Md"}},{"cell_type":"code","source":["class MyModule(nn.Module):\n","\n","    def __init__(self, in_features=128, out_features=64):\n","        super().__init__()\n","\n","        layers = [\n","            nn.Linear(in_features, in_features // 2),\n","            nn.BatchNorm1d(in_features // 2),\n","            nn.ReLU(),\n","            nn.Linear(in_features // 2, out_features),\n","            nn.BatchNorm1d(out_features),\n","        ]\n","\n","        self.layers = nn.Sequential(*layers)\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.activation(x)\n","        return x\n","\n","if __name__ == '__main__':\n","    x = torch.randn(64,128)\n","    module = MyModule()\n","    print('output size:', module(x).shape)\n","    summary(module, (64, 128))\n","    print(module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1oPzcCL_ect","executionInfo":{"status":"ok","timestamp":1654839398219,"user_tz":-480,"elapsed":5,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"06162776-bb14-4083-805d-92a6315fa84a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output size: torch.Size([64, 64])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1               [-1, 64, 64]           8,256\n","       BatchNorm1d-2               [-1, 64, 64]             128\n","              ReLU-3               [-1, 64, 64]               0\n","            Linear-4               [-1, 64, 64]           4,160\n","       BatchNorm1d-5               [-1, 64, 64]             128\n","           Sigmoid-6               [-1, 64, 64]               0\n","================================================================\n","Total params: 12,672\n","Trainable params: 12,672\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 0.19\n","Params size (MB): 0.05\n","Estimated Total Size (MB): 0.27\n","----------------------------------------------------------------\n","MyModule(\n","  (layers): Sequential(\n","    (0): Linear(in_features=128, out_features=64, bias=True)\n","    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Linear(in_features=64, out_features=64, bias=True)\n","    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (activation): Sigmoid()\n",")\n"]}]},{"cell_type":"markdown","source":["### add_module\n","\n","除了一個一個輸入以外，我們也可以使用 add_module 方法將參數註冊到網路中\n","\n","add_module 方法需要輸入兩個參數，子類名稱和子類，由於 add_module 是 Sequential 的屬性，所以我們需要先建立 Sequential 物件才能使用 add_module 方法\n","\n","\n","\n"],"metadata":{"id":"m2qFUMArHZ6d"}},{"cell_type":"code","source":["class MyModule(nn.Module):\n","\n","    def __init__(self, in_features=512, out_features=64, depth=5):\n","        super().__init__()\n","\n","        self.layers = nn.Sequential()\n","        for i in range(depth):\n","            self.layers.add_module(f'linear{i+1}', nn.Linear(in_features, in_features // 2))\n","            self.layers.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n","            in_features = in_features // 2\n","        self.layers.add_module(f'linear{depth+1}', nn.Linear(in_features, out_features))\n","\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.activation(x)\n","        return x\n","\n","if __name__ == '__main__':\n","    x = torch.randn(64,512)\n","    module = MyModule()\n","    print('output size:', module(x).shape)\n","    summary(module, (64, 512))\n","    print(module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6EO8pcfSHl8D","executionInfo":{"status":"ok","timestamp":1654839861627,"user_tz":-480,"elapsed":262,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"d2f5f06e-233c-4d17-e15b-7a13e3880d33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output size: torch.Size([64, 64])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1              [-1, 64, 256]         131,328\n","              ReLU-2              [-1, 64, 256]               0\n","            Linear-3              [-1, 64, 128]          32,896\n","              ReLU-4              [-1, 64, 128]               0\n","            Linear-5               [-1, 64, 64]           8,256\n","              ReLU-6               [-1, 64, 64]               0\n","            Linear-7               [-1, 64, 32]           2,080\n","              ReLU-8               [-1, 64, 32]               0\n","            Linear-9               [-1, 64, 16]             528\n","             ReLU-10               [-1, 64, 16]               0\n","           Linear-11               [-1, 64, 64]           1,088\n","          Sigmoid-12               [-1, 64, 64]               0\n","================================================================\n","Total params: 176,176\n","Trainable params: 176,176\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.12\n","Forward/backward pass size (MB): 0.55\n","Params size (MB): 0.67\n","Estimated Total Size (MB): 1.34\n","----------------------------------------------------------------\n","MyModule(\n","  (layers): Sequential(\n","    (linear1): Linear(in_features=512, out_features=256, bias=True)\n","    (relu1): ReLU(inplace=True)\n","    (linear2): Linear(in_features=256, out_features=128, bias=True)\n","    (relu2): ReLU(inplace=True)\n","    (linear3): Linear(in_features=128, out_features=64, bias=True)\n","    (relu3): ReLU(inplace=True)\n","    (linear4): Linear(in_features=64, out_features=32, bias=True)\n","    (relu4): ReLU(inplace=True)\n","    (linear5): Linear(in_features=32, out_features=16, bias=True)\n","    (relu5): ReLU(inplace=True)\n","    (linear6): Linear(in_features=16, out_features=64, bias=True)\n","  )\n","  (activation): Sigmoid()\n",")\n"]}]},{"cell_type":"markdown","source":["## Nested Module\n","\n","通常一個常用的網路部件會被調用很多次，我們可以先把它寫成一個物件，以後每次要用到只需要調用物件就可\n","\n","我們來看一個簡單的例子，假設我們的網路常常需要用到 ```Linear-BatchNorm-ReLU``` 的部件，那麼我就可以新定義一個物件 ```LBR``` 如下"],"metadata":{"id":"8MfAWY1zAheB"}},{"cell_type":"code","source":["class LBR(nn.Module):\n","\n","    def __init__(self, in_features, out_features):\n","        super().__init__()\n","\n","        self.linear = nn.Linear(in_features, out_features)\n","        self.bn =  nn.BatchNorm1d(out_features)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","\n","        x = self.linear(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","\n","        return x"],"metadata":{"id":"yRSdQeuLCH2D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["由於 nn 底下的函數都是 Module 的子類，所以我們可以向使用 ```nn.Linear``` 那樣直接使用 ```LBR```，不需要再用 ```nn.Parameter``` 告訴網路這是需要更新的參數"],"metadata":{"id":"56yA1ktyCHPg"}},{"cell_type":"code","source":["class MyModule(nn.Module):\n","\n","    def __init__(self, in_features=128, out_features=64):\n","        super().__init__()\n","\n","        layers = [\n","            LBR(in_features, in_features // 2),\n","            LBR(in_features // 2, out_features),\n","        ]\n","\n","        self.layers = nn.Sequential(*layers)\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.activation(x)\n","        return x\n","\n","if __name__ == '__main__':\n","    x = torch.randn(64,128)\n","    module = MyModule()\n","    print('output size:', module(x).shape)\n","    summary(module, (64, 128))\n","    print(module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qW4ps9YFAQ1W","executionInfo":{"status":"ok","timestamp":1654839399872,"user_tz":-480,"elapsed":424,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"79a82ae3-fdad-4735-b6b5-7e030b897bc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output size: torch.Size([64, 64])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1               [-1, 64, 64]           8,256\n","       BatchNorm1d-2               [-1, 64, 64]             128\n","              ReLU-3               [-1, 64, 64]               0\n","               LBR-4               [-1, 64, 64]               0\n","            Linear-5               [-1, 64, 64]           4,160\n","       BatchNorm1d-6               [-1, 64, 64]             128\n","              ReLU-7               [-1, 64, 64]               0\n","               LBR-8               [-1, 64, 64]               0\n","           Sigmoid-9               [-1, 64, 64]               0\n","================================================================\n","Total params: 12,672\n","Trainable params: 12,672\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 0.28\n","Params size (MB): 0.05\n","Estimated Total Size (MB): 0.36\n","----------------------------------------------------------------\n","MyModule(\n","  (layers): Sequential(\n","    (0): LBR(\n","      (linear): Linear(in_features=128, out_features=64, bias=True)\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (1): LBR(\n","      (linear): Linear(in_features=64, out_features=64, bias=True)\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (activation): Sigmoid()\n",")\n"]}]},{"cell_type":"markdown","source":["## ModuleList\n","\n","在 Sequential 方法中，Layer 之間是存在 **forward** 關係的，那如果我們只想蒐集某些部件，他們之間不存在連接關係呢? 這就可以用到 ModuleList 方法\n","\n","ModuleList 方法會把 Module 的子類用 list 蒐集起來，並註冊到網路底下，但是子類彼此之間是獨立的"],"metadata":{"id":"f2qbbGsaC5hX"}},{"cell_type":"code","source":["class MyModule(nn.Module):\n","\n","    def __init__(self, in_features=128, out_features=64):\n","        super().__init__()\n","\n","        self.layers = nn.ModuleList([\n","            LBR(in_features, in_features // 2),\n","            LBR(in_features // 2, out_features),\n","        ])\n","\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = self.activation(x)\n","        return x\n","\n","if __name__ == '__main__':\n","    x = torch.randn(64,128)\n","    module = MyModule()\n","    print('output size:', module(x).shape)\n","    summary(module, (64, 128))\n","    print(module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WM78hetABaQ6","executionInfo":{"status":"ok","timestamp":1654839400525,"user_tz":-480,"elapsed":3,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"792c161d-5464-4dcd-a348-3cdfc81ff5a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output size: torch.Size([64, 64])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1               [-1, 64, 64]           8,256\n","       BatchNorm1d-2               [-1, 64, 64]             128\n","              ReLU-3               [-1, 64, 64]               0\n","               LBR-4               [-1, 64, 64]               0\n","            Linear-5               [-1, 64, 64]           4,160\n","       BatchNorm1d-6               [-1, 64, 64]             128\n","              ReLU-7               [-1, 64, 64]               0\n","               LBR-8               [-1, 64, 64]               0\n","           Sigmoid-9               [-1, 64, 64]               0\n","================================================================\n","Total params: 12,672\n","Trainable params: 12,672\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 0.28\n","Params size (MB): 0.05\n","Estimated Total Size (MB): 0.36\n","----------------------------------------------------------------\n","MyModule(\n","  (layers): ModuleList(\n","    (0): LBR(\n","      (linear): Linear(in_features=128, out_features=64, bias=True)\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (1): LBR(\n","      (linear): Linear(in_features=64, out_features=64, bias=True)\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (activation): Sigmoid()\n",")\n"]}]},{"cell_type":"markdown","source":["## Conclusion\n","\n","在這一章節，我們學到了怎麼建立一個簡易的神經網路，除去一些特別的網路架構，本章節的內容可以處理大部分的網路問題了 !!\n","\n","- 自定義網路須至少包含\n","  - ```__init__():``` 網路初始化設定\n","  - ```forward():``` 前向傳播路徑\n","- ```Sequential:``` 自動將子類註冊到網路中，且子類之間有 forward 關係\n","- ```ModuleList:``` 自動將子類註冊到網路中，子類之間相互獨立\n","- ```add_module:``` Sequential 的附屬方法\n","- 自定義子類，需繼承於 ```nn.Module```"],"metadata":{"id":"CWkHBrRNJgXh"}}]}