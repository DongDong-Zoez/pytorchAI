{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"optimizer.ipynb","provenance":[],"authorship_tag":"ABX9TyPJtRX3SfqPHUr+gkcUQ/RQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Pytorch optimizer and loss function\n","\n","一旦我們建立完網路之後，我們要知道網路怎麼更新參數，所以我們必須設定一個 criterion 用來衡量模型當前的表現，也就是 loss function\n","\n","有了目標函數之後，我們可以用梯度下降的演算法 (optimizer) 去更新我們的網路參數\n"],"metadata":{"id":"FKKxZSYS3lzL"}},{"cell_type":"markdown","source":["## Pytorch Loss function\n","\n","這裡最常用的 pytorch built-in loss function，除此之外，大部分都需要自定義\n","\n","- ```nn.L1Loss:``` L1 正則損失 (絕對值差)\n","- ```nn.MSELoss:``` L2 正則損失 (平方差)\n","- ```nn.CrossEntropyLoss:``` 交叉謪，用於分類任務\n","- ```nn.NLLLoss:``` 負 log likelihood，用於分類任務\n","- ```nn.BCELoss:``` 二元交叉謪，用於二元分類\n","- ```nn.BCEWithLogitsLoss:``` Sigmoid 加上二元交叉謪\n","\n","這是其他好用套件的 loss function\n","\n","- ```LabelSmoothingLoss:``` [標籤平滑](https://github.com/PistonY/torch-toolbox)"],"metadata":{"id":"80Z8gPVbm46R"}},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnZpJZvd3hef","executionInfo":{"status":"ok","timestamp":1654852213126,"user_tz":-480,"elapsed":413,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"379769ca-1edb-40fb-bd39-ef71cf82c86c"},"outputs":[{"output_type":"stream","name":"stdout","text":["gradient of x:\n"," tensor([[-2., -2.],\n","        [-2., -2.]])\n","gradient of y:\n"," tensor([[2., 2.],\n","        [2., 2.]])\n"]}],"source":["import torch.nn as nn\n","import torch\n","from torch.autograd import Variable\n","\n","x = Variable(torch.FloatTensor([[1, 2], [3, 4]]), requires_grad=True)\n","y = Variable(torch.FloatTensor([[5, 6], [7, 8]]), requires_grad=True)\n","\n","criterion = nn.MSELoss()\n","loss = criterion(x, y)\n","loss.backward()\n","\n","print('gradient of x:\\n', x.grad)\n","print('gradient of y:\\n', y.grad)"]},{"cell_type":"markdown","source":["### Customize Loss function\n","\n","自定義 loss function 可以有很多種方法，但是你要確保梯度可以反向傳播，其中一種我比較推薦的方法是寫個 Module 的子類\n","\n","以下我們寫個自定義的 MSELoss function"],"metadata":{"id":"qjdBl49Jt2G7"}},{"cell_type":"code","source":["x.grad.zero_()\n","y.grad.zero_()\n","\n","class MSELoss(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        pass\n","\n","    def forward(self, x, y):\n","\n","        return torch.mean((x - y) ** 2) \n","\n","criterion = MSELoss()\n","loss = criterion(x, y)\n","loss.backward()\n","\n","print('gradient of x:\\n', x.grad)\n","print('gradient of y:\\n', y.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0sZmxqLpBp0","executionInfo":{"status":"ok","timestamp":1654852223549,"user_tz":-480,"elapsed":7,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"98045919-5740-43d4-835a-8688deaf6eb2"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["gradient of x:\n"," tensor([[-2., -2.],\n","        [-2., -2.]])\n","gradient of y:\n"," tensor([[2., 2.],\n","        [2., 2.]])\n"]}]},{"cell_type":"markdown","source":["在 pytorch 中梯度是會累加的，所以我們要執行\n","\n","```python\n","x.grad.zero_()\n","y.grad.zero_()\n","```\n","\n","來將之前保存的梯度清零\n","\n"],"metadata":{"id":"dFj90zgP5JSM"}},{"cell_type":"markdown","source":["## [Optimizer](https://pytorch.org/docs/stable/optim.html)\n","\n","這裡最常用的 pytorch built-in optimizer，除此之外，大部分都需要自定義\n","\n","- ```Adagrad:``` (Class Adagrad)\n","- ```Adam:``` (Class Adam)\n","- ```AdamW:``` (Class AdamW)\n","- ```LBFGS:``` (Class LBFGS)\n","- ```RMSprop:``` (Class RMSprop)\n","- ```SGD:``` (Class SGD)\n","\n","當我們完全梯度計算並且反向傳播之後，我們可以 optimizer 類底下的 step 方法來更新參數\n","\n","注意: 每次更新之前都需要把梯度清零\n","\n"],"metadata":{"id":"TcnBSPyc5XcS"}},{"cell_type":"code","source":["from torch.optim import Adam\n","\n","class MyModule(nn.Module):\n","\n","    def __init__(self, in_features=512, out_features=64, depth=5):\n","        super().__init__()\n","\n","        self.layers = nn.Sequential()\n","        for i in range(depth):\n","            self.layers.add_module(f'linear{i+1}', nn.Linear(in_features, in_features // 2))\n","            self.layers.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n","            in_features = in_features // 2\n","        self.layers.add_module(f'linear{depth+1}', nn.Linear(in_features, out_features))\n","\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.activation(x)\n","        return x\n","\n","model = MyModule()\n","\n","optimizer = Adam(model.parameters(), lr=0.01)\n","\n","if __name__ == '__main__':\n","    print(optimizer)\n","    for i in range(5):\n","        optimizer.zero_grad()\n","        optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DifiocX3kvB","executionInfo":{"status":"ok","timestamp":1654855379755,"user_tz":-480,"elapsed":3,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"b8003bbd-38c5-44a7-c09d-17d7711edba6"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Adam (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    eps: 1e-08\n","    lr: 0.01\n","    maximize: False\n","    weight_decay: 0\n",")\n"]}]},{"cell_type":"markdown","source":["### Learning Rate Schedule\n","\n","optimizer 的輸入參數有兩個，要更新的參數和學習率，learning rate scheduler 是一個可以動態調整 optimizer 學習率的方法，常用的有以下幾種"],"metadata":{"id":"3k-eTbX-EXo8"}},{"cell_type":"markdown","source":["#### ReduceLROnPlateau\n","\n","如果 loss 值一直沒有進步，或者梯度值趨近於零 (Plateau) \n","\n","**Parameter:**\n","\n","- patience (int) - 過多少個 epoch 沒有進步就降低 lr\n","- factor (float) - 下降比例 "],"metadata":{"id":"WRnoN13dTI3W"}},{"cell_type":"code","source":["LEARNING_RATE = 0.01\n","model = MyModule()\n","\n","optimizer = Adam(model.parameters(), lr = LEARNING_RATE)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2)\n","\n","for i in range(1, 15):\n","    optimizer.zero_grad()\n","    optimizer.step()\n","    print(f\"Epoch [{i}]  lr: {optimizer.param_groups[0]['lr']}\")\n","    loss = 1\n","    scheduler.step(loss) # you need to pass the value of loss for scheduler to verify if it is reducing "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXna0XuoDpXA","executionInfo":{"status":"ok","timestamp":1654860427170,"user_tz":-480,"elapsed":258,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"4b7da385-e916-413e-b358-a7f0d2080d3a"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1]  lr: 0.01\n","Epoch [2]  lr: 0.01\n","Epoch [3]  lr: 0.01\n","Epoch [4]  lr: 0.01\n","Epoch [5]  lr: 0.004\n","Epoch [6]  lr: 0.004\n","Epoch [7]  lr: 0.004\n","Epoch [8]  lr: 0.0016\n","Epoch [9]  lr: 0.0016\n","Epoch [10]  lr: 0.0016\n","Epoch [11]  lr: 0.00064\n","Epoch [12]  lr: 0.00064\n","Epoch [13]  lr: 0.00064\n","Epoch [14]  lr: 0.00025600000000000004\n"]}]},{"cell_type":"markdown","source":["#### StepLR\n","\n","每過一個指定的週期後更新學習率到指定比例 \n","\n","**Parameter:**\n","\n","- step_size (int) - 學習率週期\n","- gamma (float) - 每次週期下降比例 "],"metadata":{"id":"h0nekEDrWZvd"}},{"cell_type":"code","source":["optimizer = Adam(model.parameters(), lr = LEARNING_RATE)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma = .1)\n","\n","for i in range(1, 15):\n","    optimizer.zero_grad()\n","    optimizer.step()\n","    print(f\"Epoch [{i}]  lr: {optimizer.param_groups[0]['lr']}\")\n","    scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3ALqOiGVder","executionInfo":{"status":"ok","timestamp":1654860809308,"user_tz":-480,"elapsed":432,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"cc75b9b2-ac0e-42e6-aab5-035470d759b3"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1]  lr: 0.01\n","Epoch [2]  lr: 0.01\n","Epoch [3]  lr: 0.01\n","Epoch [4]  lr: 0.01\n","Epoch [5]  lr: 0.01\n","Epoch [6]  lr: 0.01\n","Epoch [7]  lr: 0.01\n","Epoch [8]  lr: 0.01\n","Epoch [9]  lr: 0.01\n","Epoch [10]  lr: 0.01\n","Epoch [11]  lr: 0.001\n","Epoch [12]  lr: 0.001\n","Epoch [13]  lr: 0.001\n","Epoch [14]  lr: 0.001\n"]}]},{"cell_type":"markdown","source":["#### MultiStepLR\n","\n","每過一個指定的 step 次數後更新學習率到指定比例 \n","\n","**Parameter:**\n","\n","- milestones (list) - 指定 step 次數\n","- gamma (float) - 每次到達指定 step 次數後下降比例 "],"metadata":{"id":"ZCrmHgQfXkGU"}},{"cell_type":"code","source":["optimizer = Adam(model.parameters(), lr = LEARNING_RATE)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 12], gamma = .1)\n","\n","for i in range(1, 15):\n","    optimizer.zero_grad()\n","    optimizer.step()\n","    print(f\"Epoch [{i}]  lr: {optimizer.param_groups[0]['lr']}\")\n","    scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHGqkv6eXRxh","executionInfo":{"status":"ok","timestamp":1654860816386,"user_tz":-480,"elapsed":250,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"cfcef1b5-bb62-49a6-a714-92cd24dc8550"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1]  lr: 0.01\n","Epoch [2]  lr: 0.01\n","Epoch [3]  lr: 0.01\n","Epoch [4]  lr: 0.01\n","Epoch [5]  lr: 0.01\n","Epoch [6]  lr: 0.001\n","Epoch [7]  lr: 0.001\n","Epoch [8]  lr: 0.001\n","Epoch [9]  lr: 0.001\n","Epoch [10]  lr: 0.001\n","Epoch [11]  lr: 0.001\n","Epoch [12]  lr: 0.001\n","Epoch [13]  lr: 0.0001\n","Epoch [14]  lr: 0.0001\n"]}]},{"cell_type":"markdown","source":["#### ExponentialLR\n","\n","每次 step 下降指定學習率比例\n","\n","**Parameter:**\n","\n","- gamma (float) - 每次 step 下降比例 "],"metadata":{"id":"zzv7yXnFYbCN"}},{"cell_type":"code","source":["optimizer = Adam(model.parameters(), lr = LEARNING_RATE)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = .1)\n","\n","for i in range(1, 15):\n","    optimizer.zero_grad()\n","    optimizer.step()\n","    print(f\"Epoch [{i}]  lr: {optimizer.param_groups[0]['lr']}\")\n","    scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgcyD5AZYIdN","executionInfo":{"status":"ok","timestamp":1654860820942,"user_tz":-480,"elapsed":408,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"4dea5e42-f24f-4efc-ba8b-3c83a12979fe"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1]  lr: 0.01\n","Epoch [2]  lr: 0.001\n","Epoch [3]  lr: 0.0001\n","Epoch [4]  lr: 1e-05\n","Epoch [5]  lr: 1.0000000000000002e-06\n","Epoch [6]  lr: 1.0000000000000002e-07\n","Epoch [7]  lr: 1.0000000000000004e-08\n","Epoch [8]  lr: 1.0000000000000005e-09\n","Epoch [9]  lr: 1.0000000000000006e-10\n","Epoch [10]  lr: 1.0000000000000006e-11\n","Epoch [11]  lr: 1.0000000000000006e-12\n","Epoch [12]  lr: 1.0000000000000007e-13\n","Epoch [13]  lr: 1.0000000000000008e-14\n","Epoch [14]  lr: 1.0000000000000009e-15\n"]}]},{"cell_type":"markdown","source":["#### MultiplicativeLR\n","\n","每過一個指定的 step 次數後透過自定義函數更新學習率\n","\n","**Parameter:**\n","\n","- lr_lambda  (fcn or list) - 自定義函數更新學習率"],"metadata":{"id":"biQlNd6zalom"}},{"cell_type":"code","source":["def lr_lambda(epoch):\n","    return 0.2 if epoch < 5 else 0.9\n","\n","optimizer = Adam(model.parameters(), lr = LEARNING_RATE)\n","scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lr_lambda)\n","\n","for i in range(1, 15):\n","    optimizer.zero_grad()\n","    optimizer.step()\n","    print(f\"Epoch [{i}]  lr: {optimizer.param_groups[0]['lr']}\")\n","    scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpbTHw4PY6Kl","executionInfo":{"status":"ok","timestamp":1654861863925,"user_tz":-480,"elapsed":381,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"95b13cdd-1d64-4f81-cfbb-de69572fcc42"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1]  lr: 0.01\n","Epoch [2]  lr: 0.002\n","Epoch [3]  lr: 0.0004\n","Epoch [4]  lr: 8e-05\n","Epoch [5]  lr: 1.6000000000000003e-05\n","Epoch [6]  lr: 1.4400000000000003e-05\n","Epoch [7]  lr: 1.2960000000000003e-05\n","Epoch [8]  lr: 1.1664000000000002e-05\n","Epoch [9]  lr: 1.0497600000000003e-05\n","Epoch [10]  lr: 9.447840000000002e-06\n","Epoch [11]  lr: 8.503056000000003e-06\n","Epoch [12]  lr: 7.652750400000004e-06\n","Epoch [13]  lr: 6.8874753600000035e-06\n","Epoch [14]  lr: 6.198727824000003e-06\n"]}]},{"cell_type":"markdown","source":["#### LambdaLR\n","\n","每過一個指定的 step 次數後透過自定義函數更新學習率\n","\n","**Parameter:**\n","\n","- lr_lambda (fcn or list) - 自定義函數更新學習率"],"metadata":{"id":"ixfskNEgdFR7"}},{"cell_type":"code","source":["def lr_lambda(epoch):\n","    return 0.2 if epoch < 5 else 0.9\n","\n","optimizer = Adam(model.parameters(), lr = LEARNING_RATE)\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lr_lambda)\n","\n","for i in range(1, 15):\n","    optimizer.zero_grad()\n","    optimizer.step()\n","    print(f\"Epoch [{i}]  lr: {optimizer.param_groups[0]['lr']}\")\n","    scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUP92YH9cEXB","executionInfo":{"status":"ok","timestamp":1654862531305,"user_tz":-480,"elapsed":386,"user":{"displayName":"8787 Zoez","userId":"17680164657657523368"}},"outputId":"4f140632-dd3a-40b6-fcdf-9a0c607ae309"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1]  lr: 0.002\n","Epoch [2]  lr: 0.002\n","Epoch [3]  lr: 0.002\n","Epoch [4]  lr: 0.002\n","Epoch [5]  lr: 0.002\n","Epoch [6]  lr: 0.009000000000000001\n","Epoch [7]  lr: 0.009000000000000001\n","Epoch [8]  lr: 0.009000000000000001\n","Epoch [9]  lr: 0.009000000000000001\n","Epoch [10]  lr: 0.009000000000000001\n","Epoch [11]  lr: 0.009000000000000001\n","Epoch [12]  lr: 0.009000000000000001\n","Epoch [13]  lr: 0.009000000000000001\n","Epoch [14]  lr: 0.009000000000000001\n"]}]}]}